<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-09-20 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>LlamaPi Robot - Voice chatbot with LLM and robot arm</title>
<meta name="author" content="Ping Zhou" />
<meta name="generator" content="Org Mode" />
<link rel='icon' type='image/x-icon' href='/images/favicon.ico'/>
<link rel='stylesheet' href='https://code.cdn.mozilla.net/fonts/fira.css'>
<link rel='stylesheet' href='/css/comfy_inline.css' type='text/css'/>
<script async src='https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6678218894641313' crossorigin='anonymous'></script>
<script>
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
// @license-end
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="/"> UP </a>
 |
 <a accesskey="H" href="/"> HOME </a>
</div><header id="top" class="status">
<div class='intro'>
  <h1>
    <span class='gray'>Ping's Tech Notes</span>
  </h1>
</div>
</header>
<main id="content" class="content">
<header>
<h1 class="title">LlamaPi Robot - Voice chatbot with LLM and robot arm</h1>
<p class="subtitle" role="doc-subtitle">Ping Zhou, 2024-09-20</p>
</header>
<section id="outline-container-org1eef3f3" class="outline-2">
<h2 id="org1eef3f3">Intro</h2>
<div class="outline-text-2" id="text-org1eef3f3">
<p>
Recently I built a prototype demonstrating the possibilities of <b><b>Voice + LLM + Robotics</b></b>. It is a voice chatbot running on Raspberry Pi 5 backed by the latest LLM (e.g. Llama-3.1), allowing the user to control robot arm gestures through voice interactions.
</p>

<ul class="org-ul">
<li>Backed by local LLM (Llama-3.1 8B) or cloud-based LLM.</li>
<li>Local ASR (<code>faster_whisper</code>) and TTS (<code>piper</code>).</li>
<li>Robot arm commands generated by LLM based on the context of the conversation.</li>
</ul>

<p>
The prototype won 1st and 3nd prizes at the recent <a href="https://lf-edge.atlassian.net/wiki/spaces/IA/pages/28311564/Day0+Hackathon#Results">InfiniEdge AI Hackathon</a>.
</p>

<p>
Project code available on GitHub: <a href="https://github.com/zhoupingjay/LlamaPi">https://github.com/zhoupingjay/LlamaPi</a>
</p>


<figure id="org5d3097f">
<img src="../images/2024/LlamaPi_demo.png" alt="LlamaPi_demo.png" width="400px">

</figure>
</div>
</section>


<section id="outline-container-orgb32c087" class="outline-2">
<h2 id="orgb32c087">System Setup</h2>
<div class="outline-text-2" id="text-orgb32c087">
<p>
Hardware:
</p>
<ul class="org-ul">
<li>Raspberry Pi 5, 8GB RAM</li>
<li>A toy robot arm using PWM servos</li>
<li>PWM servo hat for Raspberry Pi (<a href="https://www.waveshare.net/wiki/Servo_Driver_HAT">example</a>)</li>
<li>Push button (similar to <a href="https://github.com/zhoupingjay/llm_voice_chatbot_rpi">my previous LLM voice chatbot project</a>)</li>
</ul>

<p>
Software:
</p>
<ul class="org-ul">
<li>Raspbian OS (Debian 12) desktop</li>
</ul>
</div>
</section>

<section id="outline-container-org8ada18f" class="outline-2">
<h2 id="org8ada18f">Install</h2>
<div class="outline-text-2" id="text-org8ada18f">
</div>
<div id="outline-container-org9a67c03" class="outline-3">
<h3 id="org9a67c03">Dependencies</h3>
<div class="outline-text-3" id="text-org9a67c03">
<p>
Create a virtual environment:
</p>

<div class="org-src-container">
<pre class="src src-shell">mkdir ~/.virtualenvs/
python3 -m venv ~/.virtualenvs/llamapi
<span class="org-builtin">source</span> ~/.virtualenvs/llamapi/bin/activate
</pre>
</div>

<p>
Install dependencies:
</p>

<div class="org-src-container">
<pre class="src src-shell">sudo apt install portaudio19-dev
sudo apt install libopenblas-dev libopenblas-pthread-dev libopenblas-openmp-dev libopenblas0 libopenblas0-pthread libopenblas0-openmp
sudo apt install libopenblas64-0 libopenblas64-dev libopenblas64-pthread-dev libopenblas64-openmp-dev
sudo apt install ccache build-essential cmake
</pre>
</div>

<p>
Install Python modules:
</p>

<div class="org-src-container">
<pre class="src src-shell">pip install pyaudio wave soundfile
pip install faster_whisper numpy

<span class="org-comment-delimiter"># </span><span class="org-comment">RPi.GPIO doesn't work, use rpi-lgpio as a drop-in replacement</span>
pip uninstall RPi.GPIO
pip install rpi-lgpio

pip install opencc
pip install smbus

<span class="org-comment-delimiter"># </span><span class="org-comment">For use with OpenBLAS:</span>
<span class="org-variable-name">CMAKE_ARGS</span>=<span class="org-string">"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"</span> pip install llama-cpp-python
<span class="org-variable-name">CMAKE_ARGS</span>=<span class="org-string">"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"</span> pip install <span class="org-string">'llama-cpp-python[server]'</span>
pip install openai
</pre>
</div>

<p>
Checkout the demo code:
</p>

<div class="org-src-container">
<pre class="src src-shell">git clone https://github.com/zhoupingjay/LlamaPi.git
</pre>
</div>
</div>
</div>

<div id="outline-container-org095b74a" class="outline-3">
<h3 id="org095b74a">Setup the LLM</h3>
<div class="outline-text-3" id="text-org095b74a">
<p>
Local LLM (<a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct">Llama-3.1 8B Instruct</a>)
</p>

<ul class="org-ul">
<li>Quantize the model to 4-bit so it can fit in the 8GB RAM on Raspberry Pi. You may use the quantization tool from <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, or use the <a href="https://huggingface.co/spaces/ggml-org/gguf-my-repo">GGUF-my-repo</a> space on Hugging Face.</li>

<li>Create a llm folder under LlamaPi, and download the 4-bit quantized model (<code>.gguf</code> file) under this folder. E.g. <code>llm/meta-llama-3.1-8b-instruct-q4_k_m.gguf</code>.</li>
</ul>


<p>
Cloud-based LLM
</p>

<ul class="org-ul">
<li>I also built a demo using cloud-based LLM from <a href="https://coze.com">Coze</a>. Since Coze does not provide OpenAI-compatible APIs, a simple wrapper was developed.</li>

<li>To use the cloud-based LLM, you need to setup a bot in Coze. The prompt could be similar as the one used by the local LLM, but it could be more sophisticated as the cloud-based LLM is much more powerful.</li>

<li>Check out <code>coze_demo.py</code> for more details.</li>
</ul>
</div>
</div>

<div id="outline-container-org4236ed1" class="outline-3">
<h3 id="org4236ed1">ASR</h3>
<div class="outline-text-3" id="text-org4236ed1">
<p>
Use <code>[[https://github.com/SYSTRAN/faster-whisper][faster_whisper]]</code> installed from pip. It will download the ASR model on the first run. Subsequent runs will be all local.
</p>
</div>
</div>

<div id="outline-container-orgd061322" class="outline-3">
<h3 id="orgd061322">TTS</h3>
<div class="outline-text-3" id="text-orgd061322">
<ul class="org-ul">
<li>Use <a href="https://github.com/rhasspy/piper">piper</a> for TTS.</li>
<li>Create a <code>tts</code> folder under LlamaPi, download and extract piper in this folder.</li>
</ul>

<div class="org-src-container">
<pre class="src src-shell"><span class="org-builtin">cd</span> tts
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_arm64.tar.gz
tar zxf piper_arm64.tar.gz
</pre>
</div>

<ul class="org-ul">
<li>Download voice file: <a href="https://github.com/rhasspy/piper/blob/master/VOICES.md">https://github.com/rhasspy/piper/blob/master/VOICES.md</a> You need both <code>.onnx</code> file and <code>.json</code> file. For example: <code>en_US-amy-medium.onnx</code> and <code>en_US-amy-medium.onnx.json</code>. Create a <code>voices</code> folder under <code>LlamaPi/tts</code>, and put voice files under this folder.</li>
</ul>

<p>
The resulting directory structure would look like this:
</p>

<pre class="example">
LlamaPi
├── llm
|   └── (model .gguf file)
└── tts
    ├── piper
    |   └── (piper binaries)
    └── voices
        └── (voice files)
</pre>
</div>
</div>
</section>

<section id="outline-container-orgcf5025b" class="outline-2">
<h2 id="orgcf5025b">Usage</h2>
<div class="outline-text-2" id="text-orgcf5025b">
<p>
In your virtual environment, run the LlamaPi.py script:
</p>

<div class="org-src-container">
<pre class="src src-shell">python LlamaPi.py
</pre>
</div>

<p>
You&rsquo;ll see a window with big blue button and a text box showing the conversation.
</p>

<p>
Or if you run the cloud-based demo:
</p>

<div class="org-src-container">
<pre class="src src-shell">python coze_demo.py
</pre>
</div>

<p>
(You need to set the environment variables <code>COZE_APIKEY</code> and <code>COZE_BOTID</code> to run this demo.)
</p>

<p>
The robot uses a &ldquo;push-to-talk&rdquo; mode for interaction: Hold the button, talk, and release the button after you finish. The robot will respond with text and voice.
</p>

<p>
The robot will also generate simple robot arm commands based on the context of your conversation:
</p>

<ul class="org-ul">
<li>If you say hello to the robot, it will generate a <code>$greet</code> command;</li>
<li>If you sounds happy, it will generate a <code>$smile</code> command;</li>
<li>If you sounds negative, it will generate a <code>$pat</code> command.</li>
<li>If you ask the robot to hand you over something, it will generate a <code>$retrieve</code> command to emulate the action of retrieving something.</li>
</ul>

<p>
These simple commands will result in different gestures from the robot arm.
</p>
</div>
</section>

<section id="outline-container-orge520d5e" class="outline-2">
<h2 id="orge520d5e">Challenges and Takeaways</h2>
<div class="outline-text-2" id="text-orge520d5e">
<p>
The biggest challenge is the performance of running an 8B model on a low-power edge device like Raspberry Pi. With 4-bit quantization, I was able to fit Llama-3.1 8B on the device, but the generation speed was about 1.8 tokens/second (using llama.cpp + OpenBLAS).
</p>

<p>
Several techniques (or tricks) were used to mitigate the impact on user experience, e.g.:
</p>

<ul class="org-ul">
<li>Limit the length of system prompt and responses. This prevents the local LLM version from using more sophisticated prompts.</li>

<li>I also temporarily disabled the conversation history in local LLM version to reduce the prefill time.</li>

<li>Use streaming mode in generation, and detect &ldquo;end of sentence&rdquo; on the fly. Once a sentence is finished, I call TTS immediately to speak to the user. This allows it to sound more responsive than waiting for the entire generation to be finished.</li>
</ul>

<p>
However, I need a more fundamental solution that can resolve the performance issue running LLM locally on Raspberry Pi. My target is to achieve 10 tokens/second.
</p>

<p>
<b><b>Leveraging the VideoCore GPU on Raspberry Pi 5</b></b>.
</p>

<p>
Raspberry Pi 5 has a VideoCore GPU that supports Vulkan. llama.cpp/ggml also has a Vulkan backend (<code>ggml_vulkan.cpp</code>), making this a (seemingly) viable option.
</p>

<div class="org-src-container">
<pre class="src src-shell">sudo apt install libgulkan-0.15-0 libgulkan-dev vulkan-tools libvulkan-dev libvkfft-dev libgulkan-utils glslc
</pre>
</div>

<p>
To compile llama.cpp:
</p>
<div class="org-src-container">
<pre class="src src-shell">cmake -B build -DGGML_VULKAN=ON
</pre>
</div>

<p>
Or if using <code>llama-cpp-python</code> binding:
</p>
<div class="org-src-container">
<pre class="src src-shell"><span class="org-variable-name">CMAKE_ARGS</span>=<span class="org-string">"-DGGML_VULKAN=ON"</span> pip install <span class="org-string">'llama-cpp-python'</span>
<span class="org-variable-name">CMAKE_ARGS</span>=<span class="org-string">"-DGGML_VULKAN=ON"</span> pip install <span class="org-string">'llama-cpp-python[server]'</span>
</pre>
</div>

<p>
The VideoCore GPU on Raspberry Pi does not have enough memory for the entire model, but I can offload some of the layers to the GPU using the <code>ngl</code> argument. From my experiments, offloading 20 layers (out of the 32) could pass the initialization without OOM error.
</p>

<p>
Unfortunately, llama.cpp got stuck running the model. After some research, I tried disabling loop unrolling (by setting the <code>V3D_DEBUG</code> environment) and it seemed to go through:
</p>

<div class="org-src-container">
<pre class="src src-shell"><span class="org-variable-name">V3D_DEBUG</span>=noloopunroll ./build/bin/llama-cli -m &lt;model.gguf&gt; -p <span class="org-string">"Hello"</span> -n 50 -ngl 20 -t 4 -c 512
</pre>
</div>

<p>
However, the model generated corrupted data and it was even slower than CPU (probably because I disabled loop unrolling). :-(
</p>

<p>
I did some research and the hypothesis is that it might be something to do with the shader, which assumes 32/64 warp while Raspberry Pi has 16.
</p>

<p>
So far I didn&rsquo;t have time to look further into this. Vulkan is new to me, so debugging this issue will be a bit challenging (and fun too!). Any advice will be appreciated.
</p>

<p>
<b><b>Optimize CPU inference with LUT?</b></b>
</p>

<p>
Idea inspired by the recent <a href="https://github.com/microsoft/T-MAC">T-MAC paper</a>, which uses LUTs (look-up tables) to replace arithmetic ops. This could be especially useful for low-bit quantized models. E.g. consider a multiplication between 8-bit and 4-bit numbers. If we pre-compute all possible combinations and save the results in a 256x16 table, then multiplication can be replaced by memory accesses.
</p>

<p>
I think this LUT idea makes a lot sense and might achieve significant performance boost on Raspberry Pi. In fact, the T-MAC paper showed some promising results on Raspberry Pi 5 already.
Adopting this idea in my project will also be a direction that I&rsquo;d be interested in looking into.
</p>

<p>
<b><b>Further quantize the model to 2-bit?</b></b>
</p>

<p>
I don&rsquo;t prefer this&#x2026; If you look at the help page of llama.cpp&rsquo;s quantization tool, you&rsquo;ll see that Q2 adds a lot more ppl than Q4.
</p>

<pre class="example">
 2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B
 3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B
 8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B
 9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B
......
10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B
21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B
</pre>
</div>
</section>


<section id="outline-container-orgbb6644b" class="outline-2">
<h2 id="orgbb6644b">Conclusion and Future Work</h2>
<div class="outline-text-2" id="text-orgbb6644b">
<p>
Despite the challenges, the project successfully demonstrated the potential of <b><b>Voice + LLM + Robotics</b></b> on a low-power edge device. Lots of work still need to be done to unleash the performance of Raspberry Pi. My target is to achieve 10 tokens/second with the 8B model. If you have any ideas or suggestions, please let me know!
</p>
</div>
</section>
</main>
<footer id="postamble" class="status">
<div class='footer'>
    Generated using <a href="https://www.gnu.org/software/emacs/">Emacs</a> 29.4 (<a href="https://orgmode.org">Org</a> mode 9.6.15).<br>
     "Comfy" style from <a href='https://gitlab.com/OlMon/org-themes.git'>gitlab.com/OlMon/org-themes.git</a>
    </div>
</footer>
</body>
</html>
